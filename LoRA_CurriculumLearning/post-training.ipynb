{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈을 import 합니다.\n",
    "from numba import cuda\n",
    "\n",
    "#이후 초기화 작업을 진행해줍니다.\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.0' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    "    PeftModel, \n",
    "    PeftConfig,\n",
    ")\n",
    "\n",
    "peft_type = PeftType.LORA\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",  r=8, lora_alpha=16, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.source_len=128\n",
    "        self.epochs = 10\n",
    "        self.learning_rate=0.0004\n",
    "        self.batch_size=16\n",
    "        self.shuffle = True\n",
    "        self.seed=600\n",
    "        self.num_labels=10\n",
    "        self.data_path= '/home/work/CL/dataset/healthcare/healthcare5000.pickle'\n",
    "        self.model_path = 'klue/roberta-large'\n",
    "        # self.modelsave_path = r'C:\\Users\\user\\OneDrive - KookminUNIV\\바탕 화면\\추가사전학습\\Fine_tuning'\n",
    "        # self.loss_path = r'C:\\Users\\user\\OneDrive - KookminUNIV\\바탕 화면\\추가사전학습\\Fine_tuning'\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.bachends.cudnn.bechmark = True\n",
    "    \n",
    "    seed_everything(cfg.seed) #seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "trainset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_train.pickle')\n",
    "testset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_test.pickle')\n",
    "valset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_val.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저와 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,906,132 || all params: 338,512,916 || trainable%: 0.8584995911943283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (14): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (15): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (16): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (17): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (18): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (19): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (20): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (21): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (22): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (23): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=10, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.model_path, num_labels=cfg.num_labels, output_hidden_states=False).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model\n",
    "\n",
    "model_state_dict = torch.load(\"/home/work/CL/final_ictmodel/ict5epoch.pt\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels, tokenizer, source_len) :\n",
    "    # 내가 필요한 것들을 가져와서 선처리\n",
    "        self.data = data.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "    # 데이터 셋에서 한 개의 데이터를 가져오는 함수 정의\n",
    "    \n",
    "        text = self.data[index]\n",
    "        inputs = self.tokenizer(text,max_length=self.source_len,padding='max_length',truncation=True, return_tensors='pt')\n",
    "        # inputs = self.tokenizer.batch_encode_plus([text], max_length= self.source_len, truncation=True, padding='max_length',return_tensors='pt')\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        token_type_ids = inputs['token_type_ids'].squeeze()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        # input_ids = inputs['input_ids'][0]\n",
    "        # attention_mask = inputs['attention_mask'][0]\n",
    "        # token_type_ids = inputs['token_type_ids'][0]\n",
    "        \n",
    "        # return input_ids, attention_mask, token_type_ids, label\n",
    "        \n",
    "        inputs_dict = {\n",
    "            'input_ids' : input_ids.to(device, dtype = torch.long),\n",
    "            'attention_mask' : attention_mask.to(device, dtype = torch.long),\n",
    "            'token_type_ids': token_type_ids.to(device, dtype = torch.long),\n",
    "        }\n",
    "        label = torch.tensor(label).to(device, dtype = torch.long)\n",
    "        \n",
    "        \n",
    "        return inputs_dict, label\n",
    "    \n",
    "    def __len__(self) :\n",
    "    # 데이터 셋의 길이\n",
    "        return len(self.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(data=list([str(i) for i in trainset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(trainset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "val_data = CustomDataset(data=list([str(i) for i in valset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(valset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "\n",
    "test_data = CustomDataset(data=list([str(i) for i in testset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(testset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=True,num_workers=0)\n",
    "val_loader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, val 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, loader):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    nb_train_steps = 0\n",
    "    for _,(inputs, labels) in enumerate(loader, 0): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "        outputs = model(**inputs, labels = labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "\n",
    "        pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "        true = [label for label in labels.cpu().numpy()]\n",
    "        acc = accuracy_score(true,pred)\n",
    "        \n",
    "\n",
    "        if _%32 ==0 : #만약 인덱스가 10이 되면\n",
    "            print(f'Epoch : {epoch+1}, train_{_}_step_loss : {loss.item()}')\n",
    "            psuedo_pred = [logit.argmax().item() for logit in outputs.logits]\n",
    "            psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(psuedo_pred))/len(labels)\n",
    "            print(f'{epoch+1}_{_}_step_정확도 :{psuedo_acc}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += acc\n",
    "        nb_train_steps += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    avg_loss = total_loss/len(loader)\n",
    "    avg_acc = total_accuracy/nb_train_steps\n",
    "    t_test_avg_acc = total_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepLoss:{avg_loss}')\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepacc:{avg_acc}')\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepacc:{t_test_avg_acc}')\n",
    "    loss_dic['train_loss'].append(avg_loss)\n",
    "    loss_dic['train_acc'].append(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0,0\n",
    "    nb_eval_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for _,(inputs, labels) in enumerate(loader, 0): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "            outputs = model(**inputs, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            acc = accuracy_score(true,pred)\n",
    "            eval_accuracy += acc\n",
    "            nb_eval_steps +=1\n",
    "            if _%32 ==0 : #만약 인덱스가 10이 되면\n",
    "                print(f'Epoch : {epoch+1}, val_{_}_step_loss : {loss.item()}')\n",
    "                predicted_class_id = [logit.argmax().item() for logit in outputs.logits]\n",
    "                psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(predicted_class_id))/len(labels)\n",
    "                print(f'{epoch+1}_{_}_step_정확도 :{psuedo_acc}')\n",
    "                \n",
    "                \n",
    "    e_avg_loss = eval_loss/len(loader)\n",
    "    e_avg_acc = eval_accuracy/nb_eval_steps\n",
    "    e_test_avg_acc = eval_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepLoss:{e_avg_loss}')\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepacc:{e_avg_acc}')\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepacc:{e_test_avg_acc}')\n",
    "\n",
    "    loss_dic['validation_loss'].append(e_avg_loss)\n",
    "    loss_dic['val_acc'].append(e_avg_acc)                \n",
    "    loss_dic['epoch'].append(epoch+1)\n",
    "\n",
    "    early_stopping(e_avg_loss, model)\n",
    "    return e_avg_loss, e_test_avg_acc\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_accuracy = 0,0\n",
    "    result_dic = {'prediction':[], 'label':[]}\n",
    "    with torch.no_grad():\n",
    "        for _,(inputs, labels) in tqdm(enumerate(loader, 0)): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "            outputs = model(**inputs, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            result_dic['prediction'].append(pred)\n",
    "            result_dic['label'].append(true)                \n",
    "\n",
    "            acc = accuracy_score(true,pred)\n",
    "            test_accuracy += acc\n",
    "        \n",
    "            \n",
    "                \n",
    "    t_avg_loss = test_loss/len(loader)\n",
    "    t_avg_acc = test_accuracy/len(loader)\n",
    "    print(f'test_{_}_stepLoss:{t_avg_loss}')\n",
    "    print(f'test_{_}_stepacc:{t_avg_acc}')\n",
    "\n",
    "    \n",
    "    return t_avg_loss, t_avg_acc\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=0.0004)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.* (len(train_loader) * cfg.epochs),\n",
    "    num_training_steps=(len(train_loader) * cfg.epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0% 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, train_0_step_loss : 5.841823577880859\n",
      "1_0_step_정확도 :0.0\n",
      "Epoch : 1, train_32_step_loss : 0.9029552340507507\n",
      "1_32_step_정확도 :0.5\n",
      "Epoch : 1, train_64_step_loss : 1.004148244857788\n",
      "1_64_step_정확도 :0.5625\n",
      "Epoch : 1, train_96_step_loss : 0.37734541296958923\n",
      "1_96_step_정확도 :0.8125\n",
      "Epoch : 1, train_128_step_loss : 0.644525408744812\n",
      "1_128_step_정확도 :0.6875\n",
      "Epoch : 1, train_160_step_loss : 0.8392152786254883\n",
      "1_160_step_정확도 :0.5625\n",
      "Epoch:1, train_184_stepLoss:1.0390290160436888\n",
      "Epoch:1, train_184_stepacc:0.5977477477477477\n",
      "Epoch:1, train_184_stepacc:0.5977477477477477\n",
      "Epoch : 1, val_0_step_loss : 0.5630303621292114\n",
      "1_0_step_정확도 :0.8125\n",
      "Epoch : 1, val_32_step_loss : 1.0474905967712402\n",
      "1_32_step_정확도 :0.5\n",
      "Epoch:1, val_61_stepLoss:0.8558138219579574\n",
      "Epoch:1, val_61_stepacc:0.6651785714285715\n",
      "Epoch:1, val_61_stepacc:0.6651785714285715\n",
      "Validation loss decreased (inf --> 0.855814).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10% 1/10 [01:06<09:58, 66.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, train_0_step_loss : 0.49229398369789124\n",
      "2_0_step_정확도 :0.875\n",
      "Epoch : 2, train_32_step_loss : 0.5587621331214905\n",
      "2_32_step_정확도 :0.6875\n",
      "Epoch : 2, train_64_step_loss : 0.767170786857605\n",
      "2_64_step_정확도 :0.6875\n",
      "Epoch : 2, train_96_step_loss : 0.6348305940628052\n",
      "2_96_step_정확도 :0.75\n",
      "Epoch : 2, train_128_step_loss : 0.8111518621444702\n",
      "2_128_step_정확도 :0.5\n",
      "Epoch : 2, train_160_step_loss : 0.8131891489028931\n",
      "2_160_step_정확도 :0.75\n",
      "Epoch:2, train_184_stepLoss:0.8160477891161635\n",
      "Epoch:2, train_184_stepacc:0.6823198198198198\n",
      "Epoch:2, train_184_stepacc:0.6823198198198198\n",
      "Epoch : 2, val_0_step_loss : 0.5946020483970642\n",
      "2_0_step_정확도 :0.6875\n",
      "Epoch : 2, val_32_step_loss : 1.0809921026229858\n",
      "2_32_step_정확도 :0.5\n",
      "Epoch:2, val_61_stepLoss:0.8476836138194607\n",
      "Epoch:2, val_61_stepacc:0.6720910138248848\n",
      "Epoch:2, val_61_stepacc:0.6720910138248848\n",
      "Validation loss decreased (0.855814 --> 0.847684).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20% 2/10 [02:13<08:55, 66.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, train_0_step_loss : 0.8737021684646606\n",
      "3_0_step_정확도 :0.75\n",
      "Epoch : 3, train_32_step_loss : 0.44598084688186646\n",
      "3_32_step_정확도 :0.875\n",
      "Epoch : 3, train_64_step_loss : 0.5309821963310242\n",
      "3_64_step_정확도 :0.8125\n",
      "Epoch : 3, train_96_step_loss : 0.7992185950279236\n",
      "3_96_step_정확도 :0.5625\n",
      "Epoch : 3, train_128_step_loss : 0.7565188407897949\n",
      "3_128_step_정확도 :0.6875\n",
      "Epoch : 3, train_160_step_loss : 0.7581828236579895\n",
      "3_160_step_정확도 :0.75\n",
      "Epoch:3, train_184_stepLoss:0.699243105021683\n",
      "Epoch:3, train_184_stepacc:0.7256756756756757\n",
      "Epoch:3, train_184_stepacc:0.7256756756756757\n",
      "Epoch : 3, val_0_step_loss : 0.546745240688324\n",
      "3_0_step_정확도 :0.6875\n",
      "Epoch : 3, val_32_step_loss : 1.2529733180999756\n",
      "3_32_step_정확도 :0.625\n",
      "Epoch:3, val_61_stepLoss:0.9339469522237778\n",
      "Epoch:3, val_61_stepacc:0.6768433179723502\n",
      "Epoch:3, val_61_stepacc:0.6768433179723502\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30% 3/10 [03:17<07:37, 65.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, train_0_step_loss : 0.8018729090690613\n",
      "4_0_step_정확도 :0.625\n",
      "Epoch : 4, train_32_step_loss : 0.23802876472473145\n",
      "4_32_step_정확도 :1.0\n",
      "Epoch : 4, train_64_step_loss : 0.5362002849578857\n",
      "4_64_step_정확도 :0.75\n",
      "Epoch : 4, train_96_step_loss : 0.2788258194923401\n",
      "4_96_step_정확도 :0.875\n",
      "Epoch : 4, train_128_step_loss : 0.3589422106742859\n",
      "4_128_step_정확도 :0.8125\n",
      "Epoch : 4, train_160_step_loss : 0.527886688709259\n",
      "4_160_step_정확도 :0.8125\n",
      "Epoch:4, train_184_stepLoss:0.5887410697099325\n",
      "Epoch:4, train_184_stepacc:0.775\n",
      "Epoch:4, train_184_stepacc:0.775\n",
      "Epoch : 4, val_0_step_loss : 0.7056753039360046\n",
      "4_0_step_정확도 :0.8125\n",
      "Epoch : 4, val_32_step_loss : 1.1737422943115234\n",
      "4_32_step_정확도 :0.625\n",
      "Epoch:4, val_61_stepLoss:0.8739042733946154\n",
      "Epoch:4, val_61_stepacc:0.7033410138248848\n",
      "Epoch:4, val_61_stepacc:0.7033410138248848\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40% 4/10 [04:21<06:29, 64.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, train_0_step_loss : 0.15744531154632568\n",
      "5_0_step_정확도 :0.9375\n",
      "Epoch : 5, train_32_step_loss : 0.29539617896080017\n",
      "5_32_step_정확도 :0.875\n",
      "Epoch : 5, train_64_step_loss : 0.4475652873516083\n",
      "5_64_step_정확도 :0.875\n",
      "Epoch : 5, train_96_step_loss : 0.2963002920150757\n",
      "5_96_step_정확도 :0.875\n",
      "Epoch : 5, train_128_step_loss : 0.2538905441761017\n",
      "5_128_step_정확도 :0.9375\n",
      "Epoch : 5, train_160_step_loss : 0.5805614590644836\n",
      "5_160_step_정확도 :0.75\n",
      "Epoch:5, train_184_stepLoss:0.47102101920424283\n",
      "Epoch:5, train_184_stepacc:0.8295045045045045\n",
      "Epoch:5, train_184_stepacc:0.8295045045045045\n",
      "Epoch : 5, val_0_step_loss : 0.5448226928710938\n",
      "5_0_step_정확도 :0.875\n",
      "Epoch : 5, val_32_step_loss : 0.9941269755363464\n",
      "5_32_step_정확도 :0.625\n",
      "Epoch:5, val_61_stepLoss:0.9643971826280316\n",
      "Epoch:5, val_61_stepacc:0.7001728110599078\n",
      "Epoch:5, val_61_stepacc:0.7001728110599078\n",
      "EarlyStopping counter: 3 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50% 5/10 [05:24<05:21, 64.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, train_0_step_loss : 0.3518787920475006\n",
      "6_0_step_정확도 :0.875\n",
      "Epoch : 6, train_32_step_loss : 0.27580058574676514\n",
      "6_32_step_정확도 :0.9375\n",
      "Epoch : 6, train_64_step_loss : 0.3785160481929779\n",
      "6_64_step_정확도 :0.875\n",
      "Epoch : 6, train_96_step_loss : 0.3681534230709076\n",
      "6_96_step_정확도 :0.875\n",
      "Epoch : 6, train_128_step_loss : 0.547508955001831\n",
      "6_128_step_정확도 :0.6875\n",
      "Epoch : 6, train_160_step_loss : 0.10352615267038345\n",
      "6_160_step_정확도 :1.0\n",
      "Epoch:6, train_184_stepLoss:0.3911734319216496\n",
      "Epoch:6, train_184_stepacc:0.8497747747747748\n",
      "Epoch:6, train_184_stepacc:0.8497747747747748\n",
      "Epoch : 6, val_0_step_loss : 0.5496465563774109\n",
      "6_0_step_정확도 :0.875\n",
      "Epoch : 6, val_32_step_loss : 1.2314250469207764\n",
      "6_32_step_정확도 :0.6875\n",
      "Epoch:6, val_61_stepLoss:1.0153380334377289\n",
      "Epoch:6, val_61_stepacc:0.7138536866359446\n",
      "Epoch:6, val_61_stepacc:0.7138536866359446\n",
      "EarlyStopping counter: 4 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60% 6/10 [06:28<04:16, 64.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, train_0_step_loss : 0.418997585773468\n",
      "7_0_step_정확도 :0.8125\n",
      "Epoch : 7, train_32_step_loss : 0.4648187756538391\n",
      "7_32_step_정확도 :0.75\n",
      "Epoch : 7, train_64_step_loss : 0.6149957180023193\n",
      "7_64_step_정확도 :0.8125\n",
      "Epoch : 7, train_96_step_loss : 0.41253525018692017\n",
      "7_96_step_정확도 :0.875\n",
      "Epoch : 7, train_128_step_loss : 0.6734949946403503\n",
      "7_128_step_정확도 :0.875\n",
      "Epoch : 7, train_160_step_loss : 0.023480737581849098\n",
      "7_160_step_정확도 :1.0\n",
      "Epoch:7, train_184_stepLoss:0.3042801092001232\n",
      "Epoch:7, train_184_stepacc:0.8931306306306306\n",
      "Epoch:7, train_184_stepacc:0.8931306306306306\n",
      "Epoch : 7, val_0_step_loss : 0.5476107001304626\n",
      "7_0_step_정확도 :0.875\n",
      "Epoch : 7, val_32_step_loss : 1.2977070808410645\n",
      "7_32_step_정확도 :0.6875\n",
      "Epoch:7, val_61_stepLoss:0.99028645960554\n",
      "Epoch:7, val_61_stepacc:0.7269585253456221\n",
      "Epoch:7, val_61_stepacc:0.7269585253456221\n",
      "EarlyStopping counter: 5 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70% 7/10 [07:31<03:11, 63.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, train_0_step_loss : 0.0450185127556324\n",
      "8_0_step_정확도 :1.0\n",
      "Epoch : 8, train_32_step_loss : 0.3088394105434418\n",
      "8_32_step_정확도 :0.875\n",
      "Epoch : 8, train_64_step_loss : 0.2030695527791977\n",
      "8_64_step_정확도 :0.875\n",
      "Epoch : 8, train_96_step_loss : 0.4532814025878906\n",
      "8_96_step_정확도 :0.875\n",
      "Epoch : 8, train_128_step_loss : 0.13045990467071533\n",
      "8_128_step_정확도 :1.0\n",
      "Epoch : 8, train_160_step_loss : 0.1614382416009903\n",
      "8_160_step_정확도 :0.9375\n",
      "Epoch:8, train_184_stepLoss:0.24708728164031699\n",
      "Epoch:8, train_184_stepacc:0.9152027027027027\n",
      "Epoch:8, train_184_stepacc:0.9152027027027027\n",
      "Epoch : 8, val_0_step_loss : 0.8144076466560364\n",
      "8_0_step_정확도 :0.875\n",
      "Epoch : 8, val_32_step_loss : 1.18643319606781\n",
      "8_32_step_정확도 :0.5625\n",
      "Epoch:8, val_61_stepLoss:1.053613838409224\n",
      "Epoch:8, val_61_stepacc:0.7206221198156683\n",
      "Epoch:8, val_61_stepacc:0.7206221198156683\n",
      "EarlyStopping counter: 6 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80% 8/10 [08:35<02:07, 63.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, train_0_step_loss : 0.0728139579296112\n",
      "9_0_step_정확도 :1.0\n",
      "Epoch : 9, train_32_step_loss : 0.023651091381907463\n",
      "9_32_step_정확도 :1.0\n",
      "Epoch : 9, train_64_step_loss : 0.0775563046336174\n",
      "9_64_step_정확도 :0.9375\n",
      "Epoch : 9, train_96_step_loss : 0.012764891609549522\n",
      "9_96_step_정확도 :1.0\n",
      "Epoch : 9, train_128_step_loss : 0.0764077752828598\n",
      "9_128_step_정확도 :0.9375\n",
      "Epoch : 9, train_160_step_loss : 0.5190796256065369\n",
      "9_160_step_정확도 :0.8125\n",
      "Epoch:9, train_184_stepLoss:0.19630081563734927\n",
      "Epoch:9, train_184_stepacc:0.9331081081081081\n",
      "Epoch:9, train_184_stepacc:0.9331081081081081\n",
      "Epoch : 9, val_0_step_loss : 1.042912244796753\n",
      "9_0_step_정확도 :0.8125\n",
      "Epoch : 9, val_32_step_loss : 1.7750531435012817\n",
      "9_32_step_정확도 :0.5625\n",
      "Epoch:9, val_61_stepLoss:1.2395417370623159\n",
      "Epoch:9, val_61_stepacc:0.7128456221198156\n",
      "Epoch:9, val_61_stepacc:0.7128456221198156\n",
      "EarlyStopping counter: 7 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90% 9/10 [09:39<01:03, 63.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, train_0_step_loss : 0.13455820083618164\n",
      "10_0_step_정확도 :0.9375\n",
      "Epoch : 10, train_32_step_loss : 0.11312311142683029\n",
      "10_32_step_정확도 :0.9375\n",
      "Epoch : 10, train_64_step_loss : 0.30248376727104187\n",
      "10_64_step_정확도 :0.9375\n",
      "Epoch : 10, train_96_step_loss : 0.25365957617759705\n",
      "10_96_step_정확도 :0.875\n",
      "Epoch : 10, train_128_step_loss : 0.1339104026556015\n",
      "10_128_step_정확도 :0.9375\n",
      "Epoch : 10, train_160_step_loss : 0.025321204215288162\n",
      "10_160_step_정확도 :1.0\n",
      "Epoch:10, train_184_stepLoss:0.18919903296171814\n",
      "Epoch:10, train_184_stepacc:0.9347972972972973\n",
      "Epoch:10, train_184_stepacc:0.9347972972972973\n",
      "Epoch : 10, val_0_step_loss : 0.5065388083457947\n",
      "10_0_step_정확도 :0.8125\n",
      "Epoch : 10, val_32_step_loss : 2.0206966400146484\n",
      "10_32_step_정확도 :0.625\n",
      "Epoch:10, val_61_stepLoss:1.376514793403687\n",
      "Epoch:10, val_61_stepacc:0.7118375576036866\n",
      "Epoch:10, val_61_stepacc:0.7118375576036866\n",
      "EarlyStopping counter: 8 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [10:42<00:00, 64.25s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_dic = {'epoch':[],'train_loss':[], 'validation_loss':[],'train_acc':[],'val_acc':[]}\n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(0,cfg.epochs)):\n",
    "    train(epoch, model, optimizer, train_loader)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    validate(epoch, model, val_loader)\n",
    "\n",
    "    # if early_stopping.early_stop:\n",
    "    #     break\n",
    "    \n",
    "    torch.save(model, f'/home/work/CL/final_healthmodel/lora_shuffle_{epoch+1}epoch.pt')\n",
    "    \n",
    "    \n",
    "    # index = index + 1\n",
    "df1 = pd.DataFrame(loss_dic)\n",
    "df1.to_excel(f'/home/work/CL/final_healthmodel/lora_shuffle.xlsx', index=False)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.039029</td>\n",
       "      <td>0.855814</td>\n",
       "      <td>0.597748</td>\n",
       "      <td>0.665179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.816048</td>\n",
       "      <td>0.847684</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.672091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.699243</td>\n",
       "      <td>0.933947</td>\n",
       "      <td>0.725676</td>\n",
       "      <td>0.676843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.588741</td>\n",
       "      <td>0.873904</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.703341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.471021</td>\n",
       "      <td>0.964397</td>\n",
       "      <td>0.829505</td>\n",
       "      <td>0.700173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.391173</td>\n",
       "      <td>1.015338</td>\n",
       "      <td>0.849775</td>\n",
       "      <td>0.713854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.304280</td>\n",
       "      <td>0.990286</td>\n",
       "      <td>0.893131</td>\n",
       "      <td>0.726959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.247087</td>\n",
       "      <td>1.053614</td>\n",
       "      <td>0.915203</td>\n",
       "      <td>0.720622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.196301</td>\n",
       "      <td>1.239542</td>\n",
       "      <td>0.933108</td>\n",
       "      <td>0.712846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.189199</td>\n",
       "      <td>1.376515</td>\n",
       "      <td>0.934797</td>\n",
       "      <td>0.711838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  validation_loss  train_acc   val_acc\n",
       "0      1    1.039029         0.855814   0.597748  0.665179\n",
       "1      2    0.816048         0.847684   0.682320  0.672091\n",
       "2      3    0.699243         0.933947   0.725676  0.676843\n",
       "3      4    0.588741         0.873904   0.775000  0.703341\n",
       "4      5    0.471021         0.964397   0.829505  0.700173\n",
       "5      6    0.391173         1.015338   0.849775  0.713854\n",
       "6      7    0.304280         0.990286   0.893131  0.726959\n",
       "7      8    0.247087         1.053614   0.915203  0.720622\n",
       "8      9    0.196301         1.239542   0.933108  0.712846\n",
       "9     10    0.189199         1.376515   0.934797  0.711838"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df1['epoch'],df1['train_loss'],label='train_loss')\n",
    "plt.plot(df1['epoch'],df1['validation_loss'],label='val_loss')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['train_loss'],label='no_lora_trainloss')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['validation_loss'],label='no_lora_valloss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.plot(df1['epoch'],df1['train_acc'],label='train_acc')\n",
    "plt.plot(df1['epoch'],df1['val_acc'],label='val_acc')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['train_acc'],label='no_lora_train_acc')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['val_acc'],label='no_lora_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2번 도메인까지 학습한뒤 평가(정렬된거기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8020986364253105\n",
      "test_61_stepacc:0.6895161290322581\n",
      "1 (0.8020986364253105, 0.6895161290322581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8309270936635232\n",
      "test_61_stepacc:0.6693548387096774\n",
      "2 (0.8309270936635232, 0.6693548387096774)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8438172681677726\n",
      "test_61_stepacc:0.6985887096774194\n",
      "3 (0.8438172681677726, 0.6985887096774194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8372630318326335\n",
      "test_61_stepacc:0.7016129032258065\n",
      "4 (0.8372630318326335, 0.7016129032258065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.9015427296680789\n",
      "test_61_stepacc:0.7016129032258065\n",
      "5 (0.9015427296680789, 0.7016129032258065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.9478518145699655\n",
      "test_61_stepacc:0.7036290322580645\n",
      "6 (0.9478518145699655, 0.7036290322580645)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.9627126382002907\n",
      "test_61_stepacc:0.7258064516129032\n",
      "7 (0.9627126382002907, 0.7258064516129032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.0416877115926435\n",
      "test_61_stepacc:0.7086693548387096\n",
      "8 (1.0416877115926435, 0.7086693548387096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.1550457674649455\n",
      "test_61_stepacc:0.7207661290322581\n",
      "9 (1.1550457674649455, 0.7207661290322581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.2861649239015194\n",
      "test_61_stepacc:0.7116935483870968\n",
      "10 (1.2861649239015194, 0.7116935483870968)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/work/CL/final_healthmodel/lora_shuffle_11epoch.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m11\u001b[39m) :\n\u001b[1;32m      5\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/work/CL/final_healthmodel/lora_shuffle_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mepoch.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,infer(model\u001b[38;5;241m=\u001b[39mmodel,loader\u001b[38;5;241m=\u001b[39mtest_loader))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:776\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    774\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    778\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/work/CL/final_healthmodel/lora_shuffle_11epoch.pt'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "\n",
    "for i in range(11) :\n",
    "    save_path = f\"/home/work/CL/final_healthmodel/lora_shuffle_{i+1}epoch.pt\"\n",
    "    model = torch.load(save_path)\n",
    "\n",
    "    print(i+1,infer(model=model,loader=test_loader))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
