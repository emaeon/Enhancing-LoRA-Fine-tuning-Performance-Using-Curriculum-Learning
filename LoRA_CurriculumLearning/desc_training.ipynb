{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈을 import 합니다.\n",
    "from numba import cuda\n",
    "\n",
    "#이후 초기화 작업을 진행해줍니다.\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.0' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    "    PeftModel, \n",
    "    PeftConfig,\n",
    ")\n",
    "\n",
    "peft_type = PeftType.LORA\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",  r=8, lora_alpha=16, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/work/CL/lora_largecode2modify'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.source_len=128\n",
    "        self.epochs = 10\n",
    "        self.learning_rate=0.0005\n",
    "        self.batch_size=16\n",
    "        self.shuffle = True\n",
    "        self.seed=500\n",
    "        self.num_labels=10\n",
    "        self.data_path= '/home/work/CL/dataset/healthcare/healthcare5000.pickle'\n",
    "        self.model_path = 'klue/roberta-large'\n",
    "        # self.modelsave_path = r'C:\\Users\\user\\OneDrive - KookminUNIV\\바탕 화면\\추가사전학습\\Fine_tuning'\n",
    "        # self.loss_path = r'C:\\Users\\user\\OneDrive - KookminUNIV\\바탕 화면\\추가사전학습\\Fine_tuning'\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.bachends.cudnn.bechmark = True\n",
    "    \n",
    "    seed_everything(cfg.seed) #seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset.to_pickle('/home/work/CL/dataset/healthcare/healthcare_train.pickle')\n",
    "# testset.to_pickle('/home/work/CL/dataset/healthcare/healthcare_test.pickle')\n",
    "# valset.to_pickle('/home/work/CL/dataset/healthcare/healthcare_val.pickle')\n",
    "\n",
    "trainset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_train.pickle')\n",
    "testset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_test.pickle')\n",
    "valset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_val.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    618\n",
       "7    609\n",
       "9    583\n",
       "6    579\n",
       "5    561\n",
       "Name: 수도라벨, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset['수도라벨'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    \n",
    "    save_path = f\"/home/work/CL/dataset/healthcare/16_desc{i+1}_combined.csv\"\n",
    "    globals()['trainset{}'.format(i+1)]= pd.read_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저와 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,906,132 || all params: 338,512,916 || trainable%: 0.8584995911943283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (14): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (15): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (16): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (17): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (18): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (19): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (20): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (21): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (22): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (23): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=10, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.model_path, num_labels=cfg.num_labels, output_hidden_states=False).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model\n",
    "\n",
    "model_state_dict = torch.load(\"/home/work/CL/final_ictmodel/ict5epoch.pt\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels, tokenizer, source_len) :\n",
    "    # 내가 필요한 것들을 가져와서 선처리\n",
    "        self.data = data.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "    # 데이터 셋에서 한 개의 데이터를 가져오는 함수 정의\n",
    "    \n",
    "        text = self.data[index]\n",
    "        inputs = self.tokenizer(text,max_length=self.source_len,padding='max_length',truncation=True, return_tensors='pt')\n",
    "        # inputs = self.tokenizer.batch_encode_plus([text], max_length= self.source_len, truncation=True, padding='max_length',return_tensors='pt')\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        token_type_ids = inputs['token_type_ids'].squeeze()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        # input_ids = inputs['input_ids'][0]\n",
    "        # attention_mask = inputs['attention_mask'][0]\n",
    "        # token_type_ids = inputs['token_type_ids'][0]\n",
    "        \n",
    "        # return input_ids, attention_mask, token_type_ids, label\n",
    "        \n",
    "        inputs_dict = {\n",
    "            'input_ids' : input_ids.to(device, dtype = torch.long),\n",
    "            'attention_mask' : attention_mask.to(device, dtype = torch.long),\n",
    "            'token_type_ids': token_type_ids.to(device, dtype = torch.long),\n",
    "        }\n",
    "        label = torch.tensor(label).to(device, dtype = torch.long)\n",
    "        \n",
    "        \n",
    "        return inputs_dict, label\n",
    "    \n",
    "    def __len__(self) :\n",
    "    # 데이터 셋의 길이\n",
    "        return len(self.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = []  # 데이터프레임 리스트 초기화\n",
    "\n",
    "for i in range(1, 21):\n",
    "    trainset = globals()[f\"trainset{i}\"]  # 동적으로 변수명을 활용하여 데이터프레임 가져오기\n",
    "    dataframes.append(trainset)  # 데이터프레임 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임을 데이터셋으로 변환\n",
    "datasets = []\n",
    "for trainset in dataframes:\n",
    "    dataset = CustomDataset(data=list([str(i) for i in trainset['clean_text'].values.copy()].copy()),\n",
    "                            labels=list(trainset['수도라벨'].copy()),\n",
    "                            tokenizer=tokenizer,\n",
    "                            source_len=cfg.source_len)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 16\n",
    "data_loaders = []\n",
    "for dataset in datasets:\n",
    "    data_loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "    data_loaders.append(data_loader)\n",
    "len(data_loaders)\n",
    "# # 데이터로더 사용 예시\n",
    "# for data_loader in data_loaders:\n",
    "#     for batch in data_loader:\n",
    "#         inputs_dict, label = batch\n",
    "#         print(inputs_dict)\n",
    "#         print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = CustomDataset(data=list([str(i) for i in trainset['clean_text'].values.copy()].copy()),\n",
    "#                            labels= list(trainset['수도라벨'].copy()),\n",
    "#                            tokenizer= tokenizer,\n",
    "#                            source_len= cfg.source_len)\n",
    "\n",
    "val_data = CustomDataset(data=list([str(i) for i in valset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(valset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "\n",
    "test_data = CustomDataset(data=list([str(i) for i in testset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(testset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "val_loader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, val 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, loader):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    nb_train_steps = 0\n",
    "    for _,(inputs, labels) in enumerate(loader, 0): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "        outputs = model(**inputs, labels = labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "\n",
    "        pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "        true = [label for label in labels.cpu().numpy()]\n",
    "        acc = accuracy_score(true,pred)\n",
    "        \n",
    "\n",
    "        if _%32 ==0 : #만약 인덱스가 10이 되면\n",
    "            print(f'Epoch : {epoch+1}, train_{_}_step_loss : {loss.item()}')\n",
    "            psuedo_pred = [logit.argmax().item() for logit in outputs.logits]\n",
    "            psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(psuedo_pred))/len(labels)\n",
    "            print(f'{epoch+1}_{_}_step_정확도 :{psuedo_acc}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += acc\n",
    "        nb_train_steps += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    avg_loss = total_loss/len(loader)\n",
    "    avg_acc = total_accuracy/nb_train_steps\n",
    "    t_test_avg_acc = total_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepLoss:{avg_loss}')\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepacc:{avg_acc}')\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepacc:{t_test_avg_acc}')\n",
    "    loss_dic['train_loss'].append(avg_loss)\n",
    "    loss_dic['train_acc'].append(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, model, loader):\n",
    "   \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0,0\n",
    "    nb_eval_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for _,(inputs, labels) in enumerate(loader, 0): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "            outputs = model(**inputs, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            acc = accuracy_score(true,pred)\n",
    "            eval_accuracy += acc\n",
    "            nb_eval_steps +=1\n",
    "            if _%32 ==0 : #만약 인덱스가 10이 되면\n",
    "                print(f'Epoch : {epoch+1}, val_{_}_step_loss : {loss.item()}')\n",
    "                predicted_class_id = [logit.argmax().item() for logit in outputs.logits]\n",
    "                psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(predicted_class_id))/len(labels)\n",
    "                print(f'{epoch+1}_{_}_step_정확도 :{psuedo_acc}')\n",
    "                \n",
    "                \n",
    "    e_avg_loss = eval_loss/len(loader)\n",
    "    e_avg_acc = eval_accuracy/nb_eval_steps\n",
    "    e_test_avg_acc = eval_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepLoss:{e_avg_loss}')\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepacc:{e_avg_acc}')\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepacc:{e_test_avg_acc}')\n",
    "\n",
    "    loss_dic['validation_loss'].append(e_avg_loss)\n",
    "    loss_dic['val_acc'].append(e_avg_acc)                \n",
    "    loss_dic['epoch'].append(epoch+1)\n",
    "\n",
    "    early_stopping(e_avg_loss, model)\n",
    "    return e_avg_loss, e_test_avg_acc\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_accuracy = 0,0\n",
    "    result_dic = {'prediction':[], 'label':[]}\n",
    "    with torch.no_grad():\n",
    "        for _,(inputs, labels) in tqdm(enumerate(loader, 0)): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "            outputs = model(**inputs, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            result_dic['prediction'].append(pred)\n",
    "            result_dic['label'].append(true)                \n",
    "\n",
    "            acc = accuracy_score(true,pred)\n",
    "            test_accuracy += acc\n",
    "        \n",
    "            \n",
    "                \n",
    "    t_avg_loss = test_loss/len(loader)\n",
    "    t_avg_acc = test_accuracy/len(loader)\n",
    "    print(f'test_{_}_stepLoss:{t_avg_loss}')\n",
    "    print(f'test_{_}_stepacc:{t_avg_acc}')\n",
    "\n",
    "    \n",
    "    return t_avg_loss, t_avg_acc\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=0.0005)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.* (len(data_loader) * cfg.epochs),\n",
    "    num_training_steps=(len(data_loader) * cfg.epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0% 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, train_0_step_loss : 6.154717445373535\n",
      "1_0_step_정확도 :0.0\n",
      "Epoch : 1, train_32_step_loss : 1.4647111892700195\n",
      "1_32_step_정확도 :0.5\n",
      "Epoch : 1, train_64_step_loss : 1.7105207443237305\n",
      "1_64_step_정확도 :0.3125\n",
      "Epoch : 1, train_96_step_loss : 0.4443129003047943\n",
      "1_96_step_정확도 :0.875\n",
      "Epoch : 1, train_128_step_loss : 1.1110706329345703\n",
      "1_128_step_정확도 :0.4375\n",
      "Epoch : 1, train_160_step_loss : 1.016047477722168\n",
      "1_160_step_정확도 :0.625\n",
      "Epoch:1, train_183_stepLoss:1.1190183472050272\n",
      "Epoch:1, train_183_stepacc:0.5913722826086957\n",
      "Epoch:1, train_183_stepacc:0.5913722826086957\n",
      "Epoch : 1, val_0_step_loss : 0.4241197407245636\n",
      "1_0_step_정확도 :0.75\n",
      "Epoch : 1, val_32_step_loss : 1.2437081336975098\n",
      "1_32_step_정확도 :0.5\n",
      "Epoch:1, val_61_stepLoss:0.846976762817752\n",
      "Epoch:1, val_61_stepacc:0.6732430875576038\n",
      "Epoch:1, val_61_stepacc:0.6732430875576038\n",
      "Validation loss decreased (inf --> 0.846977).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10% 1/10 [01:06<10:00, 66.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, train_0_step_loss : 0.6986250877380371\n",
      "2_0_step_정확도 :0.8125\n",
      "Epoch : 2, train_32_step_loss : 0.9210936427116394\n",
      "2_32_step_정확도 :0.6875\n",
      "Epoch : 2, train_64_step_loss : 1.1439683437347412\n",
      "2_64_step_정확도 :0.5\n",
      "Epoch : 2, train_96_step_loss : 0.5077854990959167\n",
      "2_96_step_정확도 :0.875\n",
      "Epoch : 2, train_128_step_loss : 0.6432072520256042\n",
      "2_128_step_정확도 :0.5625\n",
      "Epoch : 2, train_160_step_loss : 0.7844031453132629\n",
      "2_160_step_정확도 :0.75\n",
      "Epoch:2, train_183_stepLoss:0.8147818603269432\n",
      "Epoch:2, train_183_stepacc:0.6681385869565217\n",
      "Epoch:2, train_183_stepacc:0.6681385869565217\n",
      "Epoch : 2, val_0_step_loss : 0.42823439836502075\n",
      "2_0_step_정확도 :0.8125\n",
      "Epoch : 2, val_32_step_loss : 1.2234052419662476\n",
      "2_32_step_정확도 :0.5\n",
      "Epoch:2, val_61_stepLoss:0.8432873617256841\n",
      "Epoch:2, val_61_stepacc:0.6876440092165899\n",
      "Epoch:2, val_61_stepacc:0.6876440092165899\n",
      "Validation loss decreased (0.846977 --> 0.843287).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20% 2/10 [02:12<08:51, 66.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, train_0_step_loss : 0.504361629486084\n",
      "3_0_step_정확도 :0.8125\n",
      "Epoch : 3, train_32_step_loss : 0.6360342502593994\n",
      "3_32_step_정확도 :0.8125\n",
      "Epoch : 3, train_64_step_loss : 0.57422935962677\n",
      "3_64_step_정확도 :0.75\n",
      "Epoch : 3, train_96_step_loss : 0.36772608757019043\n",
      "3_96_step_정확도 :0.875\n",
      "Epoch : 3, train_128_step_loss : 0.767522394657135\n",
      "3_128_step_정확도 :0.4375\n",
      "Epoch : 3, train_160_step_loss : 0.678450345993042\n",
      "3_160_step_정확도 :0.8125\n",
      "Epoch:3, train_183_stepLoss:0.680142639528798\n",
      "Epoch:3, train_183_stepacc:0.7245244565217391\n",
      "Epoch:3, train_183_stepacc:0.7245244565217391\n",
      "Epoch : 3, val_0_step_loss : 0.4726036787033081\n",
      "3_0_step_정확도 :0.75\n",
      "Epoch : 3, val_32_step_loss : 1.111824870109558\n",
      "3_32_step_정확도 :0.5\n",
      "Epoch:3, val_61_stepLoss:0.881541820062745\n",
      "Epoch:3, val_61_stepacc:0.6836117511520737\n",
      "Epoch:3, val_61_stepacc:0.6836117511520737\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30% 3/10 [03:15<07:32, 64.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, train_0_step_loss : 0.40537014603614807\n",
      "4_0_step_정확도 :0.8125\n",
      "Epoch : 4, train_32_step_loss : 0.705187976360321\n",
      "4_32_step_정확도 :0.75\n",
      "Epoch : 4, train_64_step_loss : 0.33470943570137024\n",
      "4_64_step_정확도 :0.875\n",
      "Epoch : 4, train_96_step_loss : 0.28466179966926575\n",
      "4_96_step_정확도 :0.9375\n",
      "Epoch : 4, train_128_step_loss : 0.6779015064239502\n",
      "4_128_step_정확도 :0.5\n",
      "Epoch : 4, train_160_step_loss : 0.7122340798377991\n",
      "4_160_step_정확도 :0.8125\n",
      "Epoch:4, train_183_stepLoss:0.5796241510821425\n",
      "Epoch:4, train_183_stepacc:0.7713994565217391\n",
      "Epoch:4, train_183_stepacc:0.7713994565217391\n",
      "Epoch : 4, val_0_step_loss : 0.5375186204910278\n",
      "4_0_step_정확도 :0.8125\n",
      "Epoch : 4, val_32_step_loss : 1.4627175331115723\n",
      "4_32_step_정확도 :0.5\n",
      "Epoch:4, val_61_stepLoss:0.9637107344404343\n",
      "Epoch:4, val_61_stepacc:0.7034850230414746\n",
      "Epoch:4, val_61_stepacc:0.7034850230414746\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40% 4/10 [04:18<06:23, 63.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, train_0_step_loss : 0.3978956639766693\n",
      "5_0_step_정확도 :0.8125\n",
      "Epoch : 5, train_32_step_loss : 0.6686967015266418\n",
      "5_32_step_정확도 :0.8125\n",
      "Epoch : 5, train_64_step_loss : 0.3325560390949249\n",
      "5_64_step_정확도 :0.8125\n",
      "Epoch : 5, train_96_step_loss : 0.24339018762111664\n",
      "5_96_step_정확도 :0.9375\n",
      "Epoch : 5, train_128_step_loss : 0.27447983622550964\n",
      "5_128_step_정확도 :0.9375\n",
      "Epoch : 5, train_160_step_loss : 0.5461995601654053\n",
      "5_160_step_정확도 :0.875\n",
      "Epoch:5, train_183_stepLoss:0.46157032083315047\n",
      "Epoch:5, train_183_stepacc:0.8206521739130435\n",
      "Epoch:5, train_183_stepacc:0.8206521739130435\n",
      "Epoch : 5, val_0_step_loss : 0.49108365178108215\n",
      "5_0_step_정확도 :0.8125\n",
      "Epoch : 5, val_32_step_loss : 1.258593201637268\n",
      "5_32_step_정확도 :0.5\n",
      "Epoch:5, val_61_stepLoss:1.0003354292242759\n",
      "Epoch:5, val_61_stepacc:0.6957085253456221\n",
      "Epoch:5, val_61_stepacc:0.6957085253456221\n",
      "EarlyStopping counter: 3 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50% 5/10 [05:21<05:17, 63.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, train_0_step_loss : 0.31346598267555237\n",
      "6_0_step_정확도 :0.875\n",
      "Epoch : 6, train_32_step_loss : 0.48535454273223877\n",
      "6_32_step_정확도 :0.8125\n",
      "Epoch : 6, train_64_step_loss : 0.2971269190311432\n",
      "6_64_step_정확도 :0.875\n",
      "Epoch : 6, train_96_step_loss : 0.3087986707687378\n",
      "6_96_step_정확도 :0.875\n",
      "Epoch : 6, train_128_step_loss : 0.4249598979949951\n",
      "6_128_step_정확도 :0.75\n",
      "Epoch : 6, train_160_step_loss : 0.7333903312683105\n",
      "6_160_step_정확도 :0.8125\n",
      "Epoch:6, train_183_stepLoss:0.40572501755441015\n",
      "Epoch:6, train_183_stepacc:0.8525815217391305\n",
      "Epoch:6, train_183_stepacc:0.8525815217391305\n",
      "Epoch : 6, val_0_step_loss : 0.6881532669067383\n",
      "6_0_step_정확도 :0.8125\n",
      "Epoch : 6, val_32_step_loss : 1.807522177696228\n",
      "6_32_step_정확도 :0.5\n",
      "Epoch:6, val_61_stepLoss:1.1193905889747604\n",
      "Epoch:6, val_61_stepacc:0.6949884792626728\n",
      "Epoch:6, val_61_stepacc:0.6949884792626728\n",
      "EarlyStopping counter: 4 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60% 6/10 [06:23<04:13, 63.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, train_0_step_loss : 0.1297144591808319\n",
      "7_0_step_정확도 :1.0\n",
      "Epoch : 7, train_32_step_loss : 0.25190040469169617\n",
      "7_32_step_정확도 :0.875\n",
      "Epoch : 7, train_64_step_loss : 0.4175676107406616\n",
      "7_64_step_정확도 :0.8125\n",
      "Epoch : 7, train_96_step_loss : 0.3044835031032562\n",
      "7_96_step_정확도 :0.8125\n",
      "Epoch : 7, train_128_step_loss : 0.09450764209032059\n",
      "7_128_step_정확도 :1.0\n",
      "Epoch : 7, train_160_step_loss : 0.3420051038265228\n",
      "7_160_step_정확도 :0.9375\n",
      "Epoch:7, train_183_stepLoss:0.328316757293499\n",
      "Epoch:7, train_183_stepacc:0.8787364130434783\n",
      "Epoch:7, train_183_stepacc:0.8787364130434783\n",
      "Epoch : 7, val_0_step_loss : 1.0197041034698486\n",
      "7_0_step_정확도 :0.8125\n",
      "Epoch : 7, val_32_step_loss : 2.143556594848633\n",
      "7_32_step_정확도 :0.5\n",
      "Epoch:7, val_61_stepLoss:1.27393674874498\n",
      "Epoch:7, val_61_stepacc:0.6815956221198156\n",
      "Epoch:7, val_61_stepacc:0.6815956221198156\n",
      "EarlyStopping counter: 5 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70% 7/10 [07:26<03:08, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, train_0_step_loss : 0.3337404727935791\n",
      "8_0_step_정확도 :0.75\n",
      "Epoch : 8, train_32_step_loss : 0.3214052617549896\n",
      "8_32_step_정확도 :0.8125\n",
      "Epoch : 8, train_64_step_loss : 0.6775528788566589\n",
      "8_64_step_정확도 :0.8125\n",
      "Epoch : 8, train_96_step_loss : 0.16981229186058044\n",
      "8_96_step_정확도 :0.9375\n",
      "Epoch : 8, train_128_step_loss : 0.284493625164032\n",
      "8_128_step_정확도 :0.875\n",
      "Epoch : 8, train_160_step_loss : 0.40468543767929077\n",
      "8_160_step_정확도 :0.875\n",
      "Epoch:8, train_183_stepLoss:0.30951589325685863\n",
      "Epoch:8, train_183_stepacc:0.8831521739130435\n",
      "Epoch:8, train_183_stepacc:0.8831521739130435\n",
      "Epoch : 8, val_0_step_loss : 0.8796654939651489\n",
      "8_0_step_정확도 :0.8125\n",
      "Epoch : 8, val_32_step_loss : 1.9658621549606323\n",
      "8_32_step_정확도 :0.5\n",
      "Epoch:8, val_61_stepLoss:1.4018579174674326\n",
      "Epoch:8, val_61_stepacc:0.6815956221198156\n",
      "Epoch:8, val_61_stepacc:0.6815956221198156\n",
      "EarlyStopping counter: 6 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80% 8/10 [08:29<02:05, 62.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, train_0_step_loss : 0.04209684580564499\n",
      "9_0_step_정확도 :1.0\n",
      "Epoch : 9, train_32_step_loss : 0.1649196743965149\n",
      "9_32_step_정확도 :0.875\n",
      "Epoch : 9, train_64_step_loss : 0.03368617594242096\n",
      "9_64_step_정확도 :1.0\n",
      "Epoch : 9, train_96_step_loss : 0.24863886833190918\n",
      "9_96_step_정확도 :0.875\n",
      "Epoch : 9, train_128_step_loss : 0.20365752279758453\n",
      "9_128_step_정확도 :0.9375\n",
      "Epoch : 9, train_160_step_loss : 0.14347730576992035\n",
      "9_160_step_정확도 :0.9375\n",
      "Epoch:9, train_183_stepLoss:0.2720808372326681\n",
      "Epoch:9, train_183_stepacc:0.8987771739130435\n",
      "Epoch:9, train_183_stepacc:0.8987771739130435\n",
      "Epoch : 9, val_0_step_loss : 0.5959183573722839\n",
      "9_0_step_정확도 :0.875\n",
      "Epoch : 9, val_32_step_loss : 1.9849027395248413\n",
      "9_32_step_정확도 :0.5625\n",
      "Epoch:9, val_61_stepLoss:1.3007095833459208\n",
      "Epoch:9, val_61_stepacc:0.6984447004608295\n",
      "Epoch:9, val_61_stepacc:0.6984447004608295\n",
      "EarlyStopping counter: 7 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90% 9/10 [09:31<01:02, 62.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, train_0_step_loss : 0.052665919065475464\n",
      "10_0_step_정확도 :1.0\n",
      "Epoch : 10, train_32_step_loss : 0.17200256884098053\n",
      "10_32_step_정확도 :0.9375\n",
      "Epoch : 10, train_64_step_loss : 0.13959930837154388\n",
      "10_64_step_정확도 :0.875\n",
      "Epoch : 10, train_96_step_loss : 0.23653645813465118\n",
      "10_96_step_정확도 :0.9375\n",
      "Epoch : 10, train_128_step_loss : 0.13411736488342285\n",
      "10_128_step_정확도 :0.875\n",
      "Epoch : 10, train_160_step_loss : 0.3425259292125702\n",
      "10_160_step_정확도 :0.9375\n",
      "Epoch:10, train_183_stepLoss:0.23247185201667572\n",
      "Epoch:10, train_183_stepacc:0.9184782608695652\n",
      "Epoch:10, train_183_stepacc:0.9184782608695652\n",
      "Epoch : 10, val_0_step_loss : 0.5806156992912292\n",
      "10_0_step_정확도 :0.875\n",
      "Epoch : 10, val_32_step_loss : 2.045677661895752\n",
      "10_32_step_정확도 :0.5\n",
      "Epoch:10, val_61_stepLoss:1.2916164487119643\n",
      "Epoch:10, val_61_stepacc:0.691676267281106\n",
      "Epoch:10, val_61_stepacc:0.691676267281106\n",
      "EarlyStopping counter: 8 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [10:34<00:00, 63.44s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_dic = {'epoch':[],'train_loss':[], 'validation_loss':[],'train_acc':[],'val_acc':[]}\n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(0,cfg.epochs)):\n",
    "    train(epoch, model, optimizer, data_loaders[epoch])\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    validate(epoch, model, val_loader)\n",
    "    \n",
    "    # if early_stopping.early_stop:\n",
    "    #     break\n",
    "    torch.save(model, f'/home/work/CL/final_healthmodel/lora_desc_{epoch+1}epoch.pt')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "df1 = pd.DataFrame(loss_dic)\n",
    "df1.to_excel(f'/home/work/CL/final_healthmodel/lora_desc.xlsx', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.119018</td>\n",
       "      <td>0.846977</td>\n",
       "      <td>0.591372</td>\n",
       "      <td>0.673243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.814782</td>\n",
       "      <td>0.843287</td>\n",
       "      <td>0.668139</td>\n",
       "      <td>0.687644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.680143</td>\n",
       "      <td>0.881542</td>\n",
       "      <td>0.724524</td>\n",
       "      <td>0.683612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.579624</td>\n",
       "      <td>0.963711</td>\n",
       "      <td>0.771399</td>\n",
       "      <td>0.703485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.461570</td>\n",
       "      <td>1.000335</td>\n",
       "      <td>0.820652</td>\n",
       "      <td>0.695709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.405725</td>\n",
       "      <td>1.119391</td>\n",
       "      <td>0.852582</td>\n",
       "      <td>0.694988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.328317</td>\n",
       "      <td>1.273937</td>\n",
       "      <td>0.878736</td>\n",
       "      <td>0.681596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.309516</td>\n",
       "      <td>1.401858</td>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.681596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.272081</td>\n",
       "      <td>1.300710</td>\n",
       "      <td>0.898777</td>\n",
       "      <td>0.698445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.232472</td>\n",
       "      <td>1.291616</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.691676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  validation_loss  train_acc   val_acc\n",
       "0      1    1.119018         0.846977   0.591372  0.673243\n",
       "1      2    0.814782         0.843287   0.668139  0.687644\n",
       "2      3    0.680143         0.881542   0.724524  0.683612\n",
       "3      4    0.579624         0.963711   0.771399  0.703485\n",
       "4      5    0.461570         1.000335   0.820652  0.695709\n",
       "5      6    0.405725         1.119391   0.852582  0.694988\n",
       "6      7    0.328317         1.273937   0.878736  0.681596\n",
       "7      8    0.309516         1.401858   0.883152  0.681596\n",
       "8      9    0.272081         1.300710   0.898777  0.698445\n",
       "9     10    0.232472         1.291616   0.918478  0.691676"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss_dic['epoch'],loss_dic['train_loss'],label='train_loss')\n",
    "plt.plot(loss_dic['epoch'],loss_dic['validation_loss'],label='val_loss')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['train_loss'],label='no_lora_trainloss')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['validation_loss'],label='no_lora_valloss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.plot(loss_dic['epoch'],loss_dic['train_acc'],label='train_acc')\n",
    "plt.plot(loss_dic['epoch'],loss_dic['val_acc'],label='val_acc')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['train_acc'],label='no_lora_train_acc')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['val_acc'],label='no_lora_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2번 도메인까지 학습한뒤 평가(정렬된거기준)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1에폭 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7986541649987621\n",
      "test_61_stepacc:0.6945564516129032\n",
      "1 (0.7986541649987621, 0.6945564516129032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7838105121927876\n",
      "test_61_stepacc:0.6945564516129032\n",
      "2 (0.7838105121927876, 0.6945564516129032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7739005127260762\n",
      "test_61_stepacc:0.6985887096774194\n",
      "3 (0.7739005127260762, 0.6985887096774194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8437486699511928\n",
      "test_61_stepacc:0.719758064516129\n",
      "4 (0.8437486699511928, 0.719758064516129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.9150554912705575\n",
      "test_61_stepacc:0.7026209677419355\n",
      "5 (0.9150554912705575, 0.7026209677419355)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.9974284893082034\n",
      "test_61_stepacc:0.7086693548387096\n",
      "6 (0.9974284893082034, 0.7086693548387096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.1279879428686634\n",
      "test_61_stepacc:0.6925403225806451\n",
      "7 (1.1279879428686634, 0.6925403225806451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.2595948927825498\n",
      "test_61_stepacc:0.6834677419354839\n",
      "8 (1.2595948927825498, 0.6834677419354839)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.1531955484421021\n",
      "test_61_stepacc:0.7127016129032258\n",
      "9 (1.1531955484421021, 0.7127016129032258)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:1.1943927801424457\n",
      "test_61_stepacc:0.7096774193548387\n",
      "10 (1.1943927801424457, 0.7096774193548387)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/work/CL/final_healthmodel/lora_desc_11epoch.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m) :\n\u001b[1;32m      5\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/work/CL/final_healthmodel/lora_desc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mepoch.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,infer(model\u001b[38;5;241m=\u001b[39mmodel,loader\u001b[38;5;241m=\u001b[39mtest_loader))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:776\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    774\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    778\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/work/CL/final_healthmodel/lora_desc_11epoch.pt'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "\n",
    "for i in range(20) :\n",
    "    save_path = f\"/home/work/CL/final_healthmodel/lora_desc_{i+1}epoch.pt\"\n",
    "    model = torch.load(save_path)\n",
    "\n",
    "    print(i+1,infer(model=model,loader=test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
