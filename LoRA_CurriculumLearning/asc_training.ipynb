{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 모듈을 import 합니다.\n",
    "from numba import cuda\n",
    "\n",
    "#이후 초기화 작업을 진행해줍니다.\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.0' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    "    PeftModel, \n",
    "    PeftConfig,\n",
    ")\n",
    "\n",
    "peft_type = PeftType.LORA\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\",  r=8, lora_alpha=16, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.source_len=128\n",
    "        self.epochs = 10\n",
    "        self.learning_rate=0.00007\n",
    "        self.batch_size=16\n",
    "        self.shuffle = True\n",
    "        self.seed=200\n",
    "        self.num_labels=10\n",
    "        self.data_path= '/home/work/CL/dataset/healthcare/healthcare5000.pickle'\n",
    "        self.model_path = 'klue/roberta-large'\n",
    "        # self.modelsave_path = r'C:\\Users\\user\\OneDrive - KookminUNIV\\바탕 화면\\추가사전학습\\Fine_tuning'\n",
    "        # self.loss_path = r'C:\\Users\\user\\OneDrive - KookminUNIV\\바탕 화면\\추가사전학습\\Fine_tuning'\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 랜덤시드 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.bachends.cudnn.bechmark = True\n",
    "    \n",
    "    seed_everything(cfg.seed) #seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset.to_pickle('/home/work/CL/dataset/healthcare/healthcare_train.pickle')\n",
    "# testset.to_pickle('/home/work/CL/dataset/healthcare/healthcare_test.pickle')\n",
    "# valset.to_pickle('/home/work/CL/dataset/healthcare/healthcare_val.pickle')\n",
    "\n",
    "trainset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_train.pickle')\n",
    "testset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_test.pickle')\n",
    "valset = pd.read_pickle('/home/work/CL/dataset/healthcare/healthcare_val.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    \n",
    "    save_path = f\"/home/work/CL/dataset/healthcare/16_asc{i+1}_combined.csv\"\n",
    "    globals()['trainset{}'.format(i+1)]= pd.read_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>cos_dis</th>\n",
       "      <th>수도라벨</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>나병환자의 피부도말 검사의 염색법과 판독법에 대하여 고찰한다</td>\n",
       "      <td>0.292617</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>기존 유모차 또는 휠체어를 계단 에스컬레이터 및 경사로에서 안전하게 사용할 수 있도...</td>\n",
       "      <td>0.312076</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>전라남도 중고등학생들의 우울 정도와 자살에 대한 실태를 파악하고 어떠한 요소가 영향...</td>\n",
       "      <td>0.293319</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>및 유전자조작생쥐를 이용한 면역관용파괴 및 항암효과분석</td>\n",
       "      <td>0.307755</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>기존의 물리치료 장비는 냉각치료 기능이 없거나 별도의 냉각치료 장비를 구비하여 사용</td>\n",
       "      <td>0.316436</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>주관기관 메디아나\\n 개발\\n 진단 기능의 측정 모듈 개발\\n 실시간 진단 기능\\n...</td>\n",
       "      <td>0.795272</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>보급형 질량분석 진단기기 시제품 제작 기술 개발 \\n 아래 사양을 만족하는 질량분석...</td>\n",
       "      <td>0.813591</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>일회용 내시경 시스템 시제품 제작\\n 및 채널을 구비한 벤딩 경비 내시경 시제품 제...</td>\n",
       "      <td>0.804985</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>제품의 모듈화표준화를 통한 비용 절감과 사용자의 다양한 니즈를 충족시키고 사용자 중...</td>\n",
       "      <td>0.791470</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>다양한 감염성 병원균의 신속 정확한 진단을 위한 프로브를 개발하고 이들을 이용한 통...</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2944 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text   cos_dis  수도라벨\n",
       "0                     나병환자의 피부도말 검사의 염색법과 판독법에 대하여 고찰한다  0.292617     8\n",
       "1     기존 유모차 또는 휠체어를 계단 에스컬레이터 및 경사로에서 안전하게 사용할 수 있도...  0.312076     5\n",
       "2     전라남도 중고등학생들의 우울 정도와 자살에 대한 실태를 파악하고 어떠한 요소가 영향...  0.293319     8\n",
       "3                        및 유전자조작생쥐를 이용한 면역관용파괴 및 항암효과분석  0.307755     8\n",
       "4        기존의 물리치료 장비는 냉각치료 기능이 없거나 별도의 냉각치료 장비를 구비하여 사용  0.316436     9\n",
       "...                                                 ...       ...   ...\n",
       "2939  주관기관 메디아나\\n 개발\\n 진단 기능의 측정 모듈 개발\\n 실시간 진단 기능\\n...  0.795272     9\n",
       "2940  보급형 질량분석 진단기기 시제품 제작 기술 개발 \\n 아래 사양을 만족하는 질량분석...  0.813591     9\n",
       "2941  일회용 내시경 시스템 시제품 제작\\n 및 채널을 구비한 벤딩 경비 내시경 시제품 제...  0.804985     9\n",
       "2942  제품의 모듈화표준화를 통한 비용 절감과 사용자의 다양한 니즈를 충족시키고 사용자 중...  0.791470     5\n",
       "2943  다양한 감염성 병원균의 신속 정확한 진단을 위한 프로브를 개발하고 이들을 이용한 통...  0.795100     6\n",
       "\n",
       "[2944 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>cos_dis</th>\n",
       "      <th>수도라벨</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>조직재생공학연구소는 중점연구소사업을통해손상된뼈치아신경이라는타겟조직을완벽히재생하기위한...</td>\n",
       "      <td>0.323545</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>조현병은 정신과 의사가 치료하는 질환 중에서 가장 이해하기 어려우면서도 비극적인 병...</td>\n",
       "      <td>0.317963</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>및 유전자조작생쥐를 이용한 면역관용파괴 및 항암효과분석</td>\n",
       "      <td>0.307755</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이 신생혈관형성에 미치는 효과 및 관련 기전 규명</td>\n",
       "      <td>0.298206</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>기존 유모차 또는 휠체어를 계단 에스컬레이터 및 경사로에서 안전하게 사용할 수 있도...</td>\n",
       "      <td>0.312076</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>현장진료를 위한 융합 지능형 휴대용 초음파 영상 시스템 요소 기술 개발\\n 지능형 ...</td>\n",
       "      <td>0.794468</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>피부충진 조성물인 조직수복용 재료의 연구개발을 통한 고부가가치화 실현 클린룸 시설과...</td>\n",
       "      <td>0.788493</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2941</th>\n",
       "      <td>다양한 감염성 병원균의 신속 정확한 진단을 위한 프로브를 개발하고 이들을 이용한 통...</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>제품의 모듈화표준화를 통한 비용 절감과 사용자의 다양한 니즈를 충족시키고 사용자 중...</td>\n",
       "      <td>0.791470</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>본 연구에서는 무선인체영역통신 각 전송 계층별 특성과 프로토콜을 분석하고 무선인체영...</td>\n",
       "      <td>0.790297</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2944 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text   cos_dis  수도라벨\n",
       "0     조직재생공학연구소는 중점연구소사업을통해손상된뼈치아신경이라는타겟조직을완벽히재생하기위한...  0.323545     7\n",
       "1     조현병은 정신과 의사가 치료하는 질환 중에서 가장 이해하기 어려우면서도 비극적인 병...  0.317963     8\n",
       "2                        및 유전자조작생쥐를 이용한 면역관용파괴 및 항암효과분석  0.307755     8\n",
       "3                           이 신생혈관형성에 미치는 효과 및 관련 기전 규명  0.298206     6\n",
       "4     기존 유모차 또는 휠체어를 계단 에스컬레이터 및 경사로에서 안전하게 사용할 수 있도...  0.312076     5\n",
       "...                                                 ...       ...   ...\n",
       "2939  현장진료를 위한 융합 지능형 휴대용 초음파 영상 시스템 요소 기술 개발\\n 지능형 ...  0.794468     9\n",
       "2940  피부충진 조성물인 조직수복용 재료의 연구개발을 통한 고부가가치화 실현 클린룸 시설과...  0.788493     5\n",
       "2941  다양한 감염성 병원균의 신속 정확한 진단을 위한 프로브를 개발하고 이들을 이용한 통...  0.795100     6\n",
       "2942  제품의 모듈화표준화를 통한 비용 절감과 사용자의 다양한 니즈를 충족시키고 사용자 중...  0.791470     5\n",
       "2943  본 연구에서는 무선인체영역통신 각 전송 계층별 특성과 프로토콜을 분석하고 무선인체영...  0.790297     9\n",
       "\n",
       "[2944 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저와 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,906,132 || all params: 338,512,916 || trainable%: 0.8584995911943283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (14): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (15): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (16): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (17): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (18): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (19): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (20): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (21): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (22): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (23): RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=10, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=10, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.model_path, num_labels=cfg.num_labels, output_hidden_states=False).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model\n",
    "\n",
    "model_state_dict = torch.load(\"/home/work/CL/final_ictmodel/ict5epoch.pt\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, labels, tokenizer, source_len) :\n",
    "    # 내가 필요한 것들을 가져와서 선처리\n",
    "        self.data = data.copy()\n",
    "        self.labels = labels.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "    # 데이터 셋에서 한 개의 데이터를 가져오는 함수 정의\n",
    "    \n",
    "        text = self.data[index]\n",
    "        inputs = self.tokenizer(text,max_length=self.source_len,padding='max_length',truncation=True, return_tensors='pt')\n",
    "        # inputs = self.tokenizer.batch_encode_plus([text], max_length= self.source_len, truncation=True, padding='max_length',return_tensors='pt')\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        token_type_ids = inputs['token_type_ids'].squeeze()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        # input_ids = inputs['input_ids'][0]\n",
    "        # attention_mask = inputs['attention_mask'][0]\n",
    "        # token_type_ids = inputs['token_type_ids'][0]\n",
    "        \n",
    "        # return input_ids, attention_mask, token_type_ids, label\n",
    "        \n",
    "        inputs_dict = {\n",
    "            'input_ids' : input_ids.to(device, dtype = torch.long),\n",
    "            'attention_mask' : attention_mask.to(device, dtype = torch.long),\n",
    "            'token_type_ids': token_type_ids.to(device, dtype = torch.long),\n",
    "        }\n",
    "        label = torch.tensor(label).to(device, dtype = torch.long)\n",
    "        \n",
    "        \n",
    "        return inputs_dict, label\n",
    "    \n",
    "    def __len__(self) :\n",
    "    # 데이터 셋의 길이\n",
    "        return len(self.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframes = []  # 데이터프레임 리스트 초기화\n",
    "\n",
    "for i in range(1, 21):\n",
    "    trainset = globals()[f\"trainset{i}\"]  # 동적으로 변수명을 활용하여 데이터프레임 가져오기\n",
    "    dataframes.append(trainset)  # 데이터프레임 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임을 데이터셋으로 변환\n",
    "datasets = []\n",
    "for trainset in dataframes:\n",
    "    dataset = CustomDataset(data=list([str(i) for i in trainset['clean_text'].values.copy()].copy()),\n",
    "                            labels=list(trainset['수도라벨'].copy()),\n",
    "                            tokenizer=tokenizer,\n",
    "                            source_len=cfg.source_len)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 16\n",
    "data_loaders = []\n",
    "for dataset in datasets:\n",
    "    data_loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "    data_loaders.append(data_loader)\n",
    "len(data_loaders)\n",
    "# # 데이터로더 사용 예시\n",
    "# for data_loader in data_loaders:\n",
    "#     for batch in data_loader:\n",
    "#         inputs_dict, label = batch\n",
    "#         print(inputs_dict)\n",
    "#         print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = CustomDataset(data=list([str(i) for i in trainset['clean_text'].values.copy()].copy()),\n",
    "#                            labels= list(trainset['수도라벨'].copy()),\n",
    "#                            tokenizer= tokenizer,\n",
    "#                            source_len= cfg.source_len)\n",
    "\n",
    "val_data = CustomDataset(data=list([str(i) for i in valset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(valset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "\n",
    "test_data = CustomDataset(data=list([str(i) for i in testset['clean_text'].values.copy()].copy()),\n",
    "                           labels= list(testset['수도라벨'].copy()),\n",
    "                           tokenizer= tokenizer,\n",
    "                           source_len= cfg.source_len)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "val_loader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=cfg.batch_size, shuffle=False,num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, val 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, loader):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0,0\n",
    "    nb_train_steps = 0\n",
    "    for _,(inputs, labels) in enumerate(loader, 0): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "        outputs = model(**inputs, labels = labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "\n",
    "        pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "        true = [label for label in labels.cpu().numpy()]\n",
    "        acc = accuracy_score(true,pred)\n",
    "        \n",
    "\n",
    "        if _%32 ==0 : #만약 인덱스가 10이 되면\n",
    "            print(f'Epoch : {epoch+1}, train_{_}_step_loss : {loss.item()}')\n",
    "            psuedo_pred = [logit.argmax().item() for logit in outputs.logits]\n",
    "            psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(psuedo_pred))/len(labels)\n",
    "            print(f'{epoch+1}_{_}_step_정확도 :{psuedo_acc}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += acc\n",
    "        nb_train_steps += 1\n",
    "    \n",
    "\n",
    "    \n",
    "    avg_loss = total_loss/len(loader)\n",
    "    avg_acc = total_accuracy/nb_train_steps\n",
    "    t_test_avg_acc = total_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepLoss:{avg_loss}')\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepacc:{avg_acc}')\n",
    "    print(f'Epoch:{epoch+1}, train_{_}_stepacc:{t_test_avg_acc}')\n",
    "    loss_dic['train_loss'].append(avg_loss)\n",
    "    loss_dic['train_acc'].append(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, model, loader):\n",
    "   \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0,0\n",
    "    nb_eval_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for _,(inputs, labels) in enumerate(loader, 0): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "            outputs = model(**inputs, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            acc = accuracy_score(true,pred)\n",
    "            eval_accuracy += acc\n",
    "            nb_eval_steps +=1\n",
    "            if _%32 ==0 : #만약 인덱스가 10이 되면\n",
    "                print(f'Epoch : {epoch+1}, val_{_}_step_loss : {loss.item()}')\n",
    "                predicted_class_id = [logit.argmax().item() for logit in outputs.logits]\n",
    "                psuedo_acc = np.sum(np.array(labels.to('cpu'))==np.array(predicted_class_id))/len(labels)\n",
    "                print(f'{epoch+1}_{_}_step_정확도 :{psuedo_acc}')\n",
    "                \n",
    "                \n",
    "    e_avg_loss = eval_loss/len(loader)\n",
    "    e_avg_acc = eval_accuracy/nb_eval_steps\n",
    "    e_test_avg_acc = eval_accuracy/len(loader)\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepLoss:{e_avg_loss}')\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepacc:{e_avg_acc}')\n",
    "    print(f'Epoch:{epoch+1}, val_{_}_stepacc:{e_test_avg_acc}')\n",
    "\n",
    "    loss_dic['validation_loss'].append(e_avg_loss)\n",
    "    loss_dic['val_acc'].append(e_avg_acc)                \n",
    "    loss_dic['epoch'].append(epoch+1)\n",
    "\n",
    "    early_stopping(e_avg_loss, model)\n",
    "    return e_avg_loss, e_test_avg_acc\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, loader):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_accuracy = 0,0\n",
    "    result_dic = {'prediction':[], 'label':[]}\n",
    "    with torch.no_grad():\n",
    "        for _,(inputs, labels) in tqdm(enumerate(loader, 0)): \n",
    "        #enumerate는 인덱스까지 반환하는 함수(데이터, 스타트번호=0)\n",
    "            outputs = model(**inputs, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = [logit.argmax().cpu().detach().item() for logit in outputs.logits]\n",
    "            true = [label for label in labels.cpu().numpy()]\n",
    "            result_dic['prediction'].append(pred)\n",
    "            result_dic['label'].append(true)                \n",
    "\n",
    "            acc = accuracy_score(true,pred)\n",
    "            test_accuracy += acc\n",
    "        \n",
    "            \n",
    "                \n",
    "    t_avg_loss = test_loss/len(loader)\n",
    "    t_avg_acc = test_accuracy/len(loader)\n",
    "    print(f'test_{_}_stepLoss:{t_avg_loss}')\n",
    "    print(f'test_{_}_stepacc:{t_avg_acc}')\n",
    "\n",
    "    \n",
    "    return t_avg_loss, t_avg_acc\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr=0.00007)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.* (len(data_loader) * cfg.epochs),\n",
    "    num_training_steps=(len(data_loader) * cfg.epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0% 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, train_0_step_loss : 5.483376979827881\n",
      "1_0_step_정확도 :0.0\n",
      "Epoch : 1, train_32_step_loss : 1.6565288305282593\n",
      "1_32_step_정확도 :0.4375\n",
      "Epoch : 1, train_64_step_loss : 0.9648651480674744\n",
      "1_64_step_정확도 :0.75\n",
      "Epoch : 1, train_96_step_loss : 1.2164827585220337\n",
      "1_96_step_정확도 :0.625\n",
      "Epoch : 1, train_128_step_loss : 0.904082179069519\n",
      "1_128_step_정확도 :0.75\n",
      "Epoch : 1, train_160_step_loss : 1.1916383504867554\n",
      "1_160_step_정확도 :0.5\n",
      "Epoch:1, train_183_stepLoss:1.312039766136719\n",
      "Epoch:1, train_183_stepacc:0.5278532608695652\n",
      "Epoch:1, train_183_stepacc:0.5278532608695652\n",
      "Epoch : 1, val_0_step_loss : 0.6745597124099731\n",
      "1_0_step_정확도 :0.6875\n",
      "Epoch : 1, val_32_step_loss : 0.9914320111274719\n",
      "1_32_step_정확도 :0.5\n",
      "Epoch:1, val_61_stepLoss:0.9029554008476196\n",
      "Epoch:1, val_61_stepacc:0.6725230414746544\n",
      "Epoch:1, val_61_stepacc:0.6725230414746544\n",
      "Validation loss decreased (inf --> 0.902955).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10% 1/10 [01:05<09:53, 66.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, train_0_step_loss : 1.105614423751831\n",
      "2_0_step_정확도 :0.4375\n",
      "Epoch : 2, train_32_step_loss : 0.9596810936927795\n",
      "2_32_step_정확도 :0.6875\n",
      "Epoch : 2, train_64_step_loss : 0.6816181540489197\n",
      "2_64_step_정확도 :0.75\n",
      "Epoch : 2, train_96_step_loss : 0.9248863458633423\n",
      "2_96_step_정확도 :0.625\n",
      "Epoch : 2, train_128_step_loss : 0.5860767364501953\n",
      "2_128_step_정확도 :0.75\n",
      "Epoch : 2, train_160_step_loss : 1.1528255939483643\n",
      "2_160_step_정확도 :0.5\n",
      "Epoch:2, train_183_stepLoss:0.8731419857105484\n",
      "Epoch:2, train_183_stepacc:0.6657608695652174\n",
      "Epoch:2, train_183_stepacc:0.6657608695652174\n",
      "Epoch : 2, val_0_step_loss : 0.608712911605835\n",
      "2_0_step_정확도 :0.8125\n",
      "Epoch : 2, val_32_step_loss : 0.9936693906784058\n",
      "2_32_step_정확도 :0.5\n",
      "Epoch:2, val_61_stepLoss:0.8507507885656049\n",
      "Epoch:2, val_61_stepacc:0.6715149769585254\n",
      "Epoch:2, val_61_stepacc:0.6715149769585254\n",
      "Validation loss decreased (0.902955 --> 0.850751).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20% 2/10 [02:11<08:47, 65.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, train_0_step_loss : 0.9603912830352783\n",
      "3_0_step_정확도 :0.625\n",
      "Epoch : 3, train_32_step_loss : 0.8892408013343811\n",
      "3_32_step_정확도 :0.625\n",
      "Epoch : 3, train_64_step_loss : 0.5321261286735535\n",
      "3_64_step_정확도 :0.75\n",
      "Epoch : 3, train_96_step_loss : 0.9302997589111328\n",
      "3_96_step_정확도 :0.6875\n",
      "Epoch : 3, train_128_step_loss : 0.5633578300476074\n",
      "3_128_step_정확도 :0.875\n",
      "Epoch : 3, train_160_step_loss : 1.1750035285949707\n",
      "3_160_step_정확도 :0.5625\n",
      "Epoch:3, train_183_stepLoss:0.8104086638144825\n",
      "Epoch:3, train_183_stepacc:0.6895380434782609\n",
      "Epoch:3, train_183_stepacc:0.6895380434782609\n",
      "Epoch : 3, val_0_step_loss : 0.5708364844322205\n",
      "3_0_step_정확도 :0.8125\n",
      "Epoch : 3, val_32_step_loss : 0.9989539980888367\n",
      "3_32_step_정확도 :0.5\n",
      "Epoch:3, val_61_stepLoss:0.8299394845962524\n",
      "Epoch:3, val_61_stepacc:0.6886520737327189\n",
      "Epoch:3, val_61_stepacc:0.6886520737327189\n",
      "Validation loss decreased (0.850751 --> 0.829939).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30% 3/10 [03:17<07:41, 65.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, train_0_step_loss : 0.9415677785873413\n",
      "4_0_step_정확도 :0.625\n",
      "Epoch : 4, train_32_step_loss : 0.819359540939331\n",
      "4_32_step_정확도 :0.8125\n",
      "Epoch : 4, train_64_step_loss : 0.6081475615501404\n",
      "4_64_step_정확도 :0.6875\n",
      "Epoch : 4, train_96_step_loss : 0.8805171251296997\n",
      "4_96_step_정확도 :0.6875\n",
      "Epoch : 4, train_128_step_loss : 0.5705974698066711\n",
      "4_128_step_정확도 :0.75\n",
      "Epoch : 4, train_160_step_loss : 1.0403872728347778\n",
      "4_160_step_정확도 :0.375\n",
      "Epoch:4, train_183_stepLoss:0.7639428433840689\n",
      "Epoch:4, train_183_stepacc:0.7027853260869565\n",
      "Epoch:4, train_183_stepacc:0.7027853260869565\n",
      "Epoch : 4, val_0_step_loss : 0.5694736242294312\n",
      "4_0_step_정확도 :0.8125\n",
      "Epoch : 4, val_32_step_loss : 1.0160373449325562\n",
      "4_32_step_정확도 :0.5\n",
      "Epoch:4, val_61_stepLoss:0.8206867152644742\n",
      "Epoch:4, val_61_stepacc:0.6826036866359446\n",
      "Epoch:4, val_61_stepacc:0.6826036866359446\n",
      "Validation loss decreased (0.829939 --> 0.820687).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40% 4/10 [04:25<06:38, 66.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, train_0_step_loss : 0.8486183881759644\n",
      "5_0_step_정확도 :0.6875\n",
      "Epoch : 5, train_32_step_loss : 0.7464616298675537\n",
      "5_32_step_정확도 :0.8125\n",
      "Epoch : 5, train_64_step_loss : 0.511851966381073\n",
      "5_64_step_정확도 :0.75\n",
      "Epoch : 5, train_96_step_loss : 0.8939555287361145\n",
      "5_96_step_정확도 :0.8125\n",
      "Epoch : 5, train_128_step_loss : 0.47914940118789673\n",
      "5_128_step_정확도 :0.8125\n",
      "Epoch : 5, train_160_step_loss : 0.8418219089508057\n",
      "5_160_step_정확도 :0.625\n",
      "Epoch:5, train_183_stepLoss:0.7255987165414769\n",
      "Epoch:5, train_183_stepacc:0.7167119565217391\n",
      "Epoch:5, train_183_stepacc:0.7167119565217391\n",
      "Epoch : 5, val_0_step_loss : 0.5623014569282532\n",
      "5_0_step_정확도 :0.8125\n",
      "Epoch : 5, val_32_step_loss : 1.0603777170181274\n",
      "5_32_step_정확도 :0.5625\n",
      "Epoch:5, val_61_stepLoss:0.828182341350663\n",
      "Epoch:5, val_61_stepacc:0.690668202764977\n",
      "Epoch:5, val_61_stepacc:0.690668202764977\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50% 5/10 [05:28<05:26, 65.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, train_0_step_loss : 0.9293190240859985\n",
      "6_0_step_정확도 :0.625\n",
      "Epoch : 6, train_32_step_loss : 0.7398912906646729\n",
      "6_32_step_정확도 :0.8125\n",
      "Epoch : 6, train_64_step_loss : 0.514153242111206\n",
      "6_64_step_정확도 :0.875\n",
      "Epoch : 6, train_96_step_loss : 0.7010848522186279\n",
      "6_96_step_정확도 :0.8125\n",
      "Epoch : 6, train_128_step_loss : 0.48956775665283203\n",
      "6_128_step_정확도 :0.8125\n",
      "Epoch : 6, train_160_step_loss : 0.7459892630577087\n",
      "6_160_step_정확도 :0.625\n",
      "Epoch:6, train_183_stepLoss:0.679555582449488\n",
      "Epoch:6, train_183_stepacc:0.7387907608695652\n",
      "Epoch:6, train_183_stepacc:0.7387907608695652\n",
      "Epoch : 6, val_0_step_loss : 0.5514193177223206\n",
      "6_0_step_정확도 :0.8125\n",
      "Epoch : 6, val_32_step_loss : 1.031770944595337\n",
      "6_32_step_정확도 :0.5625\n",
      "Epoch:6, val_61_stepLoss:0.8265074292017568\n",
      "Epoch:6, val_61_stepacc:0.692684331797235\n",
      "Epoch:6, val_61_stepacc:0.692684331797235\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60% 6/10 [06:31<04:18, 64.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, train_0_step_loss : 0.7781214714050293\n",
      "7_0_step_정확도 :0.6875\n",
      "Epoch : 7, train_32_step_loss : 0.5910587310791016\n",
      "7_32_step_정확도 :0.875\n",
      "Epoch : 7, train_64_step_loss : 0.5048305988311768\n",
      "7_64_step_정확도 :0.8125\n",
      "Epoch : 7, train_96_step_loss : 0.7826232314109802\n",
      "7_96_step_정확도 :0.75\n",
      "Epoch : 7, train_128_step_loss : 0.4012504816055298\n",
      "7_128_step_정확도 :0.875\n",
      "Epoch : 7, train_160_step_loss : 0.7548288702964783\n",
      "7_160_step_정확도 :0.6875\n",
      "Epoch:7, train_183_stepLoss:0.6395574826747179\n",
      "Epoch:7, train_183_stepacc:0.7550951086956522\n",
      "Epoch:7, train_183_stepacc:0.7550951086956522\n",
      "Epoch : 7, val_0_step_loss : 0.5317433476448059\n",
      "7_0_step_정확도 :0.8125\n",
      "Epoch : 7, val_32_step_loss : 1.0576623678207397\n",
      "7_32_step_정확도 :0.5625\n",
      "Epoch:7, val_61_stepLoss:0.8339506284363808\n",
      "Epoch:7, val_61_stepacc:0.7067972350230415\n",
      "Epoch:7, val_61_stepacc:0.7067972350230415\n",
      "EarlyStopping counter: 3 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70% 7/10 [07:34<03:12, 64.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, train_0_step_loss : 0.9559739232063293\n",
      "8_0_step_정확도 :0.625\n",
      "Epoch : 8, train_32_step_loss : 0.6482027173042297\n",
      "8_32_step_정확도 :0.75\n",
      "Epoch : 8, train_64_step_loss : 0.4869060814380646\n",
      "8_64_step_정확도 :0.8125\n",
      "Epoch : 8, train_96_step_loss : 0.7299744486808777\n",
      "8_96_step_정확도 :0.75\n",
      "Epoch : 8, train_128_step_loss : 0.4846477806568146\n",
      "8_128_step_정확도 :0.6875\n",
      "Epoch : 8, train_160_step_loss : 0.8469576239585876\n",
      "8_160_step_정확도 :0.625\n",
      "Epoch:8, train_183_stepLoss:0.605334603511121\n",
      "Epoch:8, train_183_stepacc:0.7700407608695652\n",
      "Epoch:8, train_183_stepacc:0.7700407608695652\n",
      "Epoch : 8, val_0_step_loss : 0.5134943723678589\n",
      "8_0_step_정확도 :0.8125\n",
      "Epoch : 8, val_32_step_loss : 1.0137461423873901\n",
      "8_32_step_정확도 :0.625\n",
      "Epoch:8, val_61_stepLoss:0.8590270869674221\n",
      "Epoch:8, val_61_stepacc:0.693692396313364\n",
      "Epoch:8, val_61_stepacc:0.693692396313364\n",
      "EarlyStopping counter: 4 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80% 8/10 [08:38<02:07, 63.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, train_0_step_loss : 0.8478246927261353\n",
      "9_0_step_정확도 :0.6875\n",
      "Epoch : 9, train_32_step_loss : 0.5204020738601685\n",
      "9_32_step_정확도 :0.875\n",
      "Epoch : 9, train_64_step_loss : 0.469413161277771\n",
      "9_64_step_정확도 :0.875\n",
      "Epoch : 9, train_96_step_loss : 0.6337759494781494\n",
      "9_96_step_정확도 :0.75\n",
      "Epoch : 9, train_128_step_loss : 0.3990711569786072\n",
      "9_128_step_정확도 :0.8125\n",
      "Epoch : 9, train_160_step_loss : 0.6000588536262512\n",
      "9_160_step_정확도 :0.8125\n",
      "Epoch:9, train_183_stepLoss:0.5402555998981647\n",
      "Epoch:9, train_183_stepacc:0.7951766304347826\n",
      "Epoch:9, train_183_stepacc:0.7951766304347826\n",
      "Epoch : 9, val_0_step_loss : 0.478189617395401\n",
      "9_0_step_정확도 :0.8125\n",
      "Epoch : 9, val_32_step_loss : 1.0987154245376587\n",
      "9_32_step_정확도 :0.625\n",
      "Epoch:9, val_61_stepLoss:0.869854875629948\n",
      "Epoch:9, val_61_stepacc:0.7017569124423962\n",
      "Epoch:9, val_61_stepacc:0.7017569124423962\n",
      "EarlyStopping counter: 5 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90% 9/10 [09:41<01:03, 63.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, train_0_step_loss : 0.8637111783027649\n",
      "10_0_step_정확도 :0.75\n",
      "Epoch : 10, train_32_step_loss : 0.6177163124084473\n",
      "10_32_step_정확도 :0.8125\n",
      "Epoch : 10, train_64_step_loss : 0.42402923107147217\n",
      "10_64_step_정확도 :0.8125\n",
      "Epoch : 10, train_96_step_loss : 0.6062757968902588\n",
      "10_96_step_정확도 :0.75\n",
      "Epoch : 10, train_128_step_loss : 0.2491777241230011\n",
      "10_128_step_정확도 :1.0\n",
      "Epoch : 10, train_160_step_loss : 0.6918525695800781\n",
      "10_160_step_정확도 :0.8125\n",
      "Epoch:10, train_183_stepLoss:0.513040801388738\n",
      "Epoch:10, train_183_stepacc:0.8094429347826086\n",
      "Epoch:10, train_183_stepacc:0.8094429347826086\n",
      "Epoch : 10, val_0_step_loss : 0.5386799573898315\n",
      "10_0_step_정확도 :0.875\n",
      "Epoch : 10, val_32_step_loss : 1.1353110074996948\n",
      "10_32_step_정확도 :0.625\n",
      "Epoch:10, val_61_stepLoss:0.9069706264042086\n",
      "Epoch:10, val_61_stepacc:0.6997407834101382\n",
      "Epoch:10, val_61_stepacc:0.6997407834101382\n",
      "EarlyStopping counter: 6 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [10:44<00:00, 64.47s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_dic = {'epoch':[],'train_loss':[], 'validation_loss':[],'train_acc':[],'val_acc':[]}\n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(0,cfg.epochs)):\n",
    "    train(epoch, model, optimizer, data_loaders[epoch])\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    validate(epoch, model, val_loader)\n",
    "    \n",
    "    # if early_stopping.early_stop:\n",
    "    #     break\n",
    "    torch.save(model, f'/home/work/CL/final_healthmodel/lora_asc_{epoch+1}epoch.pt')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "df1 = pd.DataFrame(loss_dic)\n",
    "df1.to_excel(f'/home/work/CL/final_healthmodel/lora_asc.xlsx', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.312040</td>\n",
       "      <td>0.902955</td>\n",
       "      <td>0.527853</td>\n",
       "      <td>0.672523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.873142</td>\n",
       "      <td>0.850751</td>\n",
       "      <td>0.665761</td>\n",
       "      <td>0.671515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.810409</td>\n",
       "      <td>0.829939</td>\n",
       "      <td>0.689538</td>\n",
       "      <td>0.688652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.763943</td>\n",
       "      <td>0.820687</td>\n",
       "      <td>0.702785</td>\n",
       "      <td>0.682604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.725599</td>\n",
       "      <td>0.828182</td>\n",
       "      <td>0.716712</td>\n",
       "      <td>0.690668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.679556</td>\n",
       "      <td>0.826507</td>\n",
       "      <td>0.738791</td>\n",
       "      <td>0.692684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.639557</td>\n",
       "      <td>0.833951</td>\n",
       "      <td>0.755095</td>\n",
       "      <td>0.706797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.605335</td>\n",
       "      <td>0.859027</td>\n",
       "      <td>0.770041</td>\n",
       "      <td>0.693692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.540256</td>\n",
       "      <td>0.869855</td>\n",
       "      <td>0.795177</td>\n",
       "      <td>0.701757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.513041</td>\n",
       "      <td>0.906971</td>\n",
       "      <td>0.809443</td>\n",
       "      <td>0.699741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  validation_loss  train_acc   val_acc\n",
       "0      1    1.312040         0.902955   0.527853  0.672523\n",
       "1      2    0.873142         0.850751   0.665761  0.671515\n",
       "2      3    0.810409         0.829939   0.689538  0.688652\n",
       "3      4    0.763943         0.820687   0.702785  0.682604\n",
       "4      5    0.725599         0.828182   0.716712  0.690668\n",
       "5      6    0.679556         0.826507   0.738791  0.692684\n",
       "6      7    0.639557         0.833951   0.755095  0.706797\n",
       "7      8    0.605335         0.859027   0.770041  0.693692\n",
       "8      9    0.540256         0.869855   0.795177  0.701757\n",
       "9     10    0.513041         0.906971   0.809443  0.699741"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(df1['epoch'],df1['train_loss'],label='train_loss')\n",
    "plt.plot(df1['epoch'],df1['validation_loss'],label='val_loss')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['train_loss'],label='no_lora_trainloss')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['validation_loss'],label='no_lora_valloss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.plot(df1['epoch'],df1['train_acc'],label='train_acc')\n",
    "plt.plot(df1['epoch'],df1['val_acc'],label='val_acc')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['train_acc'],label='no_lora_train_acc')\n",
    "# plt.plot(loss_dic['epoch'],nolora_df['val_acc'],label='no_lora_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오름차순의 추론\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8800442699463137\n",
      "test_61_stepacc:0.6703629032258065\n",
      "1 (0.8800442699463137, 0.6703629032258065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8117406272119091\n",
      "test_61_stepacc:0.6945564516129032\n",
      "2 (0.8117406272119091, 0.6945564516129032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7861387003814021\n",
      "test_61_stepacc:0.7056451612903226\n",
      "3 (0.7861387003814021, 0.7056451612903226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7775404664777941\n",
      "test_61_stepacc:0.7096774193548387\n",
      "4 (0.7775404664777941, 0.7096774193548387)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7710047378655402\n",
      "test_61_stepacc:0.7268145161290323\n",
      "5 (0.7710047378655402, 0.7268145161290323)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7672452215225466\n",
      "test_61_stepacc:0.717741935483871\n",
      "6 (0.7672452215225466, 0.717741935483871)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7706577955715118\n",
      "test_61_stepacc:0.7116935483870968\n",
      "7 (0.7706577955715118, 0.7116935483870968)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.7894127111761801\n",
      "test_61_stepacc:0.7046370967741935\n",
      "8 (0.7894127111761801, 0.7046370967741935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8042786222311759\n",
      "test_61_stepacc:0.7106854838709677\n",
      "9 (0.8042786222311759, 0.7106854838709677)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_61_stepLoss:0.8184656286912579\n",
      "test_61_stepacc:0.7147177419354839\n",
      "10 (0.8184656286912579, 0.7147177419354839)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/work/CL/final_healthmodel/lora_asc_11epoch.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m) :\n\u001b[1;32m      5\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/work/CL/final_healthmodel/lora_asc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mepoch.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,infer(model\u001b[38;5;241m=\u001b[39mmodel,loader\u001b[38;5;241m=\u001b[39mtest_loader))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:776\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    774\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    778\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/work/CL/final_healthmodel/lora_asc_11epoch.pt'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)\n",
    "\n",
    "for i in range(20) :\n",
    "    save_path = f\"/home/work/CL/final_healthmodel/lora_asc_{i+1}epoch.pt\"\n",
    "    model = torch.load(save_path)\n",
    "\n",
    "    print(i+1,infer(model=model,loader=test_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.14 (NGC 22.12/Python 3.8) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
